---
title: "STAT 4360 - Project 2"
author: "Yebom Kim"
date: "2024-09-26"
output:
  pdf_document: default
  word_document: default
---
## Section 1
```{r block0}
# 1-a)
cat("With the graphs we got, we can see most of wines have high clarity  and less aroma. I think they are in inverse relationship. Also body and clarity 
has proportion relationship. High body wines have high clarity. ")

# 1-e)
wine_data <- read.table("/Users/springkim/Downloads/wine.txt", header=TRUE)
final_model <- lm(Quality ~ Aroma + Body + Region, data=wine_data)
summary(final_model)

cat("Other questions are on the down side with the codes")
```
## Section 2 for Question 1
```{r block1}
# Load the data
wine_data <- read.table("/Users/springkim/Downloads/wine.txt", header=TRUE)

# Explore the dataset
summary(wine_data)
str(wine_data)

# Convert Region to a factor (qualitative predictor)
wine_data$Region <- as.factor(wine_data$Region)

# View the first few rows of data
head(wine_data)
```

```{r block3}
## Problem 1-a
boxplot(Quality ~ Region, data=wine_data, main="Quality by Region")
hist(wine_data$Clarity, main="Clarity", xlab="Clarity")
hist(wine_data$Aroma, main="Aroma", xlab="Aroma")
hist(wine_data$Body, main="Body", xlab="Body")
hist(wine_data$Flavor, main="Flavor", xlab="Flavor")
hist(wine_data$Oakiness, main="Oakiness", xlab="Oakiness")
```

```{r block4}
## Problem 1-b
# Fit simple linear regression models for each predictor
model_clarity <- lm(Quality ~ Clarity, data=wine_data)
model_aroma <- lm(Quality ~ Aroma, data=wine_data)
model_body <- lm(Quality ~ Body, data=wine_data)
model_flavor <- lm(Quality ~ Flavor, data=wine_data)
model_oakiness <- lm(Quality ~ Oakiness, data=wine_data)
model_region <- lm(Quality ~ Region, data=wine_data)

# Display the summary of each model
summary(model_clarity)
summary(model_aroma)
summary(model_body)
summary(model_flavor)
summary(model_oakiness)
summary(model_region)

# Create scatter plots with regression lines for each predictor
par(mfrow=c(2, 3)) # Arrange the plots in a grid

plot(wine_data$Clarity, wine_data$Quality, main="Clarity vs Quality", xlab="Clarity", ylab="Quality")
abline(model_clarity, col="red")

plot(wine_data$Aroma, wine_data$Quality, main="Aroma vs Quality", xlab="Aroma", ylab="Quality")
abline(model_aroma, col="red")

plot(wine_data$Body, wine_data$Quality, main="Body vs Quality", xlab="Body", ylab="Quality")
abline(model_body, col="red")

plot(wine_data$Flavor, wine_data$Quality, main="Flavor vs Quality", xlab="Flavor", ylab="Quality")
abline(model_flavor, col="red")

plot(wine_data$Oakiness, wine_data$Quality, main="Oakiness vs Quality", xlab="Oakiness", ylab="Quality")
abline(model_oakiness, col="red")

plot(wine_data$Region, wine_data$Quality, main="Region vs Quality", xlab="Region", ylab="Quality")
abline(model_region, col="red")
```

```{r block5}
## Problem 1-c
# Fit a multiple regression model using all predictors
multiple_model <- lm(Quality ~ Clarity + Aroma + Body + Flavor + Oakiness + Region, data=wine_data)

# Display the summary of the multiple regression model
summary(multiple_model)
```
```{r block6}
## Problem 1-d
# Start with full model including all predictors and interaction between Region and others
full_model <- lm(Quality ~ Clarity + Aroma + Body + Flavor + Oakiness + Region + 
                 Region:Clarity + Region:Aroma + Region:Body + Region:Flavor + Region:Oakiness, data=wine_data)

# Perform backward stepwise selection to simplify the model
library(MASS)
final_model <- stepAIC(full_model, direction="backward")

# Summary of the final model
summary(final_model)

# Diagnostic plots for the final model
par(mfrow=c(2,2))
plot(final_model)

```



```{r block7}
## Problem 1-f
# Calculate means of other predictors
mean_clarity <- mean(wine_data$Clarity)
mean_aroma <- mean(wine_data$Aroma)
mean_body <- mean(wine_data$Body)
mean_flavor <- mean(wine_data$Flavor)
mean_oakiness <- mean(wine_data$Oakiness)

# Create a new data frame for prediction
new_data <- data.frame(Clarity=mean_clarity, Aroma=mean_aroma, Body=mean_body, Flavor=mean_flavor, Oakiness=mean_oakiness, Region=factor(1, levels=c(1, 2, 3)))

# Predict Quality with 95% prediction interval
pred <- predict(final_model, newdata=new_data, interval="prediction", level=0.95)
pred

# Predict Quality with 95% confidence interval for mean response
conf <- predict(final_model, newdata=new_data, interval="confidence", level=0.95)
conf
```
## Section 2 for Question 2
```{r block8}
# Load the data
diabetes_data <- read.csv("/Users/springkim/Downloads/diabetes.csv", header=TRUE)

# Explore the dataset
summary(diabetes_data)
str(diabetes_data)

# Check for missing values
colSums(is.na(diabetes_data))

# View first few rows
head(diabetes_data)
```

```{r block9}
## 2-a
# Correlation matrix to assess relationships between numeric variables
cor(diabetes_data[, -9]) # Exclude 'Outcome' for correlation

# Histograms to check the distribution of each variable
hist(diabetes_data$Pregnancies, main="Pregnancies", xlab="Pregnancies")
hist(diabetes_data$Glucose, main="Glucose", xlab="Glucose")
hist(diabetes_data$BloodPressure, main="Blood Pressure", xlab="Blood Pressure")
hist(diabetes_data$SkinThickness, main="Skin Thickness", xlab="Skin Thickness")
hist(diabetes_data$Insulin, main="Insulin", xlab="Insulin")
hist(diabetes_data$BMI, main="BMI", xlab="BMI")
hist(diabetes_data$DiabetesPedigreeFunction, main="Diabetes Pedigree Function", xlab="Pedigree Function")
hist(diabetes_data$Age, main="Age", xlab="Age")

# Boxplots to check for outliers
boxplot(diabetes_data$Glucose, main="Glucose Levels")
boxplot(diabetes_data$Insulin, main="Insulin Levels")

# Plot Outcome distribution
table(diabetes_data$Outcome)
barplot(table(diabetes_data$Outcome), main="Outcome Distribution", xlab="Outcome", ylab="Count")
```

```{r block10}
## 2-b
## 1. Perform LDA
# Load the MASS library for LDA
library(MASS)

# Perform LDA
lda_model <- lda(Outcome ~ ., data=diabetes_data)

# View the LDA model
lda_model

## 2. Make Prediction
# Predict using the LDA model
lda_pred <- predict(lda_model)

# Posterior probabilities
lda_prob <- lda_pred$posterior[,2]

# Predictions based on 0.5 cutoff
lda_class <- ifelse(lda_prob > 0.5, 1, 0)

# Create confusion matrix
table(Predicted = lda_class, Actual = diabetes_data$Outcome)

## 3. Compute Confusion Matrix, Sensitivity, Specificity, and Misclassification Rate
# Confusion matrix
conf_matrix <- table(Predicted = lda_class, Actual = diabetes_data$Outcome)

# Sensitivity (True Positive Rate)
sensitivity <- conf_matrix[2,2] / (conf_matrix[2,2] + conf_matrix[1,2])

# Specificity (True Negative Rate)
specificity <- conf_matrix[1,1] / (conf_matrix[1,1] + conf_matrix[2,1])

# Overall misclassification rate
misclass_rate <- (conf_matrix[1,2] + conf_matrix[2,1]) / sum(conf_matrix)

# Output the metrics
sensitivity
specificity
misclass_rate

## 4. Plot the ROC Curve:
# Load the pROC package to plot the ROC curve
library(pROC)

# Generate ROC curve
lda_roc <- roc(diabetes_data$Outcome, lda_prob)

# Plot ROC curve
plot(lda_roc, col="blue", main="ROC Curve for LDA")
```

```{r block11}
## 2-c
## 1. Perform QDA
# Perform QDA
qda_model <- qda(Outcome ~ ., data=diabetes_data)

# View the QDA model
qda_model

## 2. Make Predictions and Evaluate
# Predict using the QDA model
qda_pred <- predict(qda_model)

# Posterior probabilities
qda_prob <- qda_pred$posterior[,2]

# Predictions based on 0.5 cutoff
qda_class <- ifelse(qda_prob > 0.5, 1, 0)

# Create confusion matrix
table(Predicted = qda_class, Actual = diabetes_data$Outcome)

## 3. Compute Confusion Matrix, Sensitivity, Specificity, and Misclassification Rate:
# Confusion matrix
conf_matrix_qda <- table(Predicted = qda_class, Actual = diabetes_data$Outcome)

# Sensitivity (True Positive Rate)
sensitivity_qda <- conf_matrix_qda[2,2] / (conf_matrix_qda[2,2] + conf_matrix_qda[1,2])

# Specificity (True Negative Rate)
specificity_qda <- conf_matrix_qda[1,1] / (conf_matrix_qda[1,1] + conf_matrix_qda[2,1])

# Overall misclassification rate
misclass_rate_qda <- (conf_matrix_qda[1,2] + conf_matrix_qda[2,1]) / sum(conf_matrix_qda)

# Output the metrics
sensitivity_qda
specificity_qda
misclass_rate_qda

## 4. Plot the ROC Curve for QDA:
# Generate ROC curve for QDA
qda_roc <- roc(diabetes_data$Outcome, qda_prob)

# Plot ROC curve
plot(qda_roc, col="red", main="ROC Curve for QDA")
```

```{r block12}
## 2-d
# Optimal cutoff based on ROC curve
optimal_cutoff <- coords(lda_roc, "best", ret="threshold")
optimal_cutoff
```
```{r block13}
## Question 3
# 3-a
# Load required package
library(MASS)  # for multivariate normal sampling

# Problem parameters
p <- 10
sigma <- 1
mu <- rep(1, p)  # vector of all 1's
N <- 1000  # number of observations

# Function to compute the James-Stein estimator
JS_estimator <- function(Y, p, sigma) {
  norm_Y_sq <- sum(Y^2)  # squared L2 norm of Y
  shrinkage_factor <- 1 - (p - 2) * sigma^2 / norm_Y_sq
  return(shrinkage_factor * Y)
}

# Initialize matrices to store estimates
JS_estimates <- matrix(0, nrow=N, ncol=p)
MLE_estimates <- matrix(0, nrow=N, ncol=p)

# Simulate N observations and compute JS and MLE estimates
set.seed(123)  # for reproducibility
for (i in 1:N) {
  Y_i <- mvrnorm(1, mu=mu, Sigma=sigma^2 * diag(p))  # generate Y_i
  JS_estimates[i, ] <- JS_estimator(Y_i, p, sigma)  # compute JS estimate
  MLE_estimates[i, ] <- Y_i  # MLE is just the observation itself
}

# Compute empirical bias
bias_JS <- norm(colMeans(JS_estimates) - mu, type="2")
bias_MLE <- norm(colMeans(MLE_estimates) - mu, type="2")

# Compute empirical risk
risk_JS <- mean(apply(JS_estimates, 1, function(est) norm(est - mu, type="2")^2))
risk_MLE <- mean(apply(MLE_estimates, 1, function(est) norm(est - mu, type="2")^2))

# Print results
cat("Bias of James-Stein estimator:", bias_JS, "\n")
cat("Bias of MLE estimator:", bias_MLE, "\n")
cat("Risk of James-Stein estimator:", risk_JS, "\n")
cat("Risk of MLE estimator:", risk_MLE, "\n")

```
```{r block14}
# 3-b
# Varying mu as a * [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
a_values <- 1:10
risk_JS_a <- numeric(length(a_values))
risk_MLE_a <- numeric(length(a_values))

for (j in 1:length(a_values)) {
  a <- a_values[j]
  mu_a <- a * mu  # new mu

  # Recompute JS and MLE estimates
  JS_estimates_a <- matrix(0, nrow=N, ncol=p)
  MLE_estimates_a <- matrix(0, nrow=N, ncol=p)

  for (i in 1:N) {
    Y_i <- mvrnorm(1, mu=mu_a, Sigma=sigma^2 * diag(p))
    JS_estimates_a[i, ] <- JS_estimator(Y_i, p, sigma)
    MLE_estimates_a[i, ] <- Y_i
  }

  # Compute risk for both estimators
  risk_JS_a[j] <- mean(apply(JS_estimates_a, 1, function(est) norm(est - mu_a, type="2")^2))
  risk_MLE_a[j] <- mean(apply(MLE_estimates_a, 1, function(est) norm(est - mu_a, type="2")^2))
}

# Plot the risk vs a
plot(a_values, risk_JS_a, type="b", col="blue", ylim=range(c(risk_JS_a, risk_MLE_a)), ylab="Risk", xlab="a", main="Risk of Estimators vs a")
lines(a_values, risk_MLE_a, type="b", col="red")
legend("topleft", legend=c("James-Stein", "MLE"), col=c("blue", "red"), lty=1, pch=1)
```

```{r block2}
# 3-c
# Varying sigma
sigma_values <- c(0.1, 0.5, 2, 5, 10)
risk_JS_sigma <- numeric(length(sigma_values))
risk_MLE_sigma <- numeric(length(sigma_values))

for (j in 1:length(sigma_values)) {
  sigma <- sigma_values[j]  # new sigma

  # Recompute JS and MLE estimates
  JS_estimates_sigma <- matrix(0, nrow=N, ncol=p)
  MLE_estimates_sigma <- matrix(0, nrow=N, ncol=p)

  for (i in 1:N) {
    Y_i <- mvrnorm(1, mu=mu, Sigma=sigma^2 * diag(p))
    JS_estimates_sigma[i, ] <- JS_estimator(Y_i, p, sigma)
    MLE_estimates_sigma[i, ] <- Y_i
  }

  # Compute risk for both estimators
  risk_JS_sigma[j] <- mean(apply(JS_estimates_sigma, 1, function(est) norm(est - mu, type="2")^2))
  risk_MLE_sigma[j] <- mean(apply(MLE_estimates_sigma, 1, function(est) norm(est - mu, type="2")^2))
}

# Plot the risk vs sigma
plot(sigma_values, risk_JS_sigma, type="b", col="blue", ylim=range(c(risk_JS_sigma, risk_MLE_sigma)), ylab="Risk", xlab="sigma", main="Risk of Estimators vs sigma")
lines(sigma_values, risk_MLE_sigma, type="b", col="red")
legend("topleft", legend=c("James-Stein", "MLE"), col=c("blue", "red"), lty=1, pch=1)
```